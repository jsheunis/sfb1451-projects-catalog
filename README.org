#+title: SFB 1451 publication catalog

Website: https://psychoinformatics-de.github.io/sfb1451-projects-catalog

* Repository overview
The catalog is placed in the =docs= directory (to allow other repository content at root level),
and served to GitHub pages from there.

* Required extensions
The following DataLad extensions are required to generate the catalog:
[[https://github.com/datalad/datalad-next][DataLad-next]] is used to interact with WebDAV remotes,
[[https://github.com/datalad/datalad-metalad][DataLad-metalad]] provides metadata extraction,
[[https://github.com/mslw/datalad-wackyextra][DataLad-wackyextra]] provides custom metadata extractors and translators,
[[https://github.com/datalad/datalad-catalog][DataLad-catalog]] is used to generate the catalog.

* Generation
Project datasets are tracked in the =Projects= superdataset.

** To obtain a subdataset:
#+begin_src bash
  datalad clone -d . webdavs://<base URL>/Projects/<Project>/<subfolder> <Project>
#+end_src
(using project names as folder names, because these would be displayed as subdataset names).

Tip: until [[https://github.com/datalad/datalad-next/issues/108][datalad-next/issues/108]] makes it automatic, storage remote can be reconfigured with:
#+begin_src bash
  git annex initremote my-sciebo-storage --private --sameas <name or uuid> exporttree=yes type=webdav url="<url>"
#+end_src

** Extract, translate, and add metadata
Extraction:
#+begin_src bash
  datalad meta-extract -d Projects metalad_core >! extracted.jsonl
  datalad meta-extract -d Projects metalad_studyminimeta >> extracted.jsonl
  datalad meta-extract -d Projects/XYZ we_cff >> extracted.jsonl
  datalad meta-extract -d Projects/XYZ we_ris >> extracted.jsonl
  datalad meta-extract -d Projects/XYZ we_nbib >> extracted.jsonl
#+end_src
Translation:
#+begin_src bash
  datalad wacky-translate -i extracted.jsonl -o translated.jsonl
#+end_src
Addition:
#+begin_src bash
  datalad catalog add -c catalog/docs -m translated.jsonl
  datalad catalog set-super -c catalog/docs -i <id> -v <version>
#+end_src

* Generation - advanced

** Use meta-conduct for several extractors

Attached =project_extract_pipeline.json= bundles `we_cff`, `we_ris`, and `we_nbib` extractors:

#+begin_src bash
  datalad -f json meta-conduct catalog/project_extract_pipeline.json \
	  traverser.top_level_dir=Projects/XYZ | \
      jq -c '.pipeline_data.result.metadata | .[].metadata_record' >> extracted.jsonl
#+end_src

** Extract from a single subdataset under a project

When running extractors, run those for user-provided metadata first, and metalad_core last.
This way, user-provided authorship information will take precedence (due to current configuration).
The metalad_core extractor will contain subdataset url if the subdataset was installed from that url.

#+begin_src bash
  # metalad core for project dataset
  datalad meta-extract -d Projects/XYZ metalad_core >> extracted.jsonl
  # user-provided metadata (e.g. authors)
  datalad meta-extract -d Projects/XYZ/subds datacite_gin >> extracted.jsonl
  # metalad core for subdataset (dataset url)
  datalad meta-extract -d Projects/XYZ/subds metalad_core >> extracted.jsonl
#+end_src
