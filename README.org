#+title: SFB 1451 publication catalog

Website: https://psychoinformatics-de.github.io/sfb1451-projects-catalog

* Repository overview
The catalog is placed in the =docs= directory (to allow other repository content at root level),
and served to GitHub pages from there.

* Related repositories:

- SFB superdataset: [[https://github.com/sfb1451/all-projects][sfb1451/all-projects]]
- Utilities for handling tabby files: [[https://github.com/sfb1451/tabby-utils][sfb1451/tabby-utils]]
- Custom extractors & translators: [[https://github.com/mslw/datalad-wackyextra][mslw/datalad-wackyextra]]

* Required extensions
The following DataLad extensions are required to generate the catalog:
[[https://github.com/datalad/datalad-next][DataLad-next]] is used to interact with WebDAV remotes,
[[https://github.com/datalad/datalad-metalad][DataLad-metalad]] provides metadata extraction,
[[https://github.com/mslw/datalad-wackyextra][DataLad-wackyextra]] provides custom metadata extractors and translators,
[[https://github.com/datalad/datalad-catalog][DataLad-catalog]] is used to generate the catalog.

* Dataset locations
Project datasets are tracked in the =Projects= superdataset. GitHub mirror: https://github.com/sfb1451/all-projects

** Obtaining a subdataset from Sciebo:
#+begin_src bash
  datalad clone -d . webdavs://<base URL>/Projects/<Project>/<subfolder> <Project>
#+end_src
(using project names as folder names, because these would be displayed as subdataset names).

Tip: until [[https://github.com/datalad/datalad-next/issues/108][datalad-next/issues/108]] makes it automatic, storage remote can be reconfigured with:
#+begin_src bash
  git annex initremote my-sciebo-storage --private --sameas <name or uuid> exporttree=yes type=webdav url="<url>"
#+end_src

or with clone url substitution - check:

#+begin_src bash
  datalad configuration  | grep 'datalad.clone.url-substitute'
#+end_src

to get some examples how you can alter all such URLs at once.

* Generation

Utility scripts are provided in the code directory. These are:

- =extract_project.py=: extract metadata from a project's superdataset.
  Conducts project_extract_pipeline.json and translates the result.
  This includes metalad_core, CFF, and bibliography (ris/nbib/crossref) extractors.
  Extracted and translated metadata are written to the specified directory
  as two separate files (project.jsonl, project.cat.jsonl).
- =extract_file.py=: similar to the above, but creates file listing (file metadata)
- =list_files.py=: replacement for =extract_file= based on basic git, git-annex, and stat calls
- =extract_selected.py=: this one targets both project's subdatasets and the sfb superdataset.
  It allows selection of extractors to be used.
  For sfb superdataset, metadata can be added to the catalog and its home page updated.
  Otherwise, metadata will be written to a file.
- =inject_metadata=: this covers addition of metadata declared in this repo (see =manually_entered.toml=),
  such as keywords or funding, and not present in the datasets.

See command line help or source code for usage instructions.
